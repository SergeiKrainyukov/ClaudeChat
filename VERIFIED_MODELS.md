# Проверенные модели Hugging Face Router API

## Статус: Январь 2025

Этот список содержит модели, которые **точно работают** с Hugging Face Router API.

## ✅ Модели по умолчанию в приложении

### Модель 1: meta-llama/Llama-3.2-3B-Instruct
- **Размер:** 3B параметров
- **От:** Meta
- **Описание:** Современная instruction-tuned модель
- **Скорость:** Быстрая
- **Качество:** Отличное для размера
- **Использование:** Универсальная генерация текста

### Модель 2: Qwen/Qwen2.5-3B-Instruct
- **Размер:** 3B параметров
- **От:** Alibaba (Qwen Team)
- **Описание:** Instruction-tuned версия Qwen 2.5
- **Скорость:** Быстрая
- **Качество:** Отличное, поддержка многих языков
- **Использование:** Универсальная, мультиязычная

## ✅ Проверенные альтернативы

### Легкие модели (быстрые)

1. **meta-llama/Llama-3.2-1B-Instruct**
   - 1B параметров
   - Очень быстрая
   - Подходит для простых задач

2. **Qwen/Qwen2.5-1.5B-Instruct**
   - 1.5B параметров
   - Хороший баланс скорости/качества
   - Мультиязычная

3. **google/gemma-2-2b-it**
   - 2B параметров
   - От Google
   - Хорошее качество

### Средние модели (лучшее качество)

1. **meta-llama/Llama-3.2-3B-Instruct** ⭐ (по умолчанию)
   - Отличный баланс
   - Рекомендуется

2. **Qwen/Qwen2.5-7B-Instruct**
   - 7B параметров
   - Высокое качество
   - Может быть медленнее на бесплатном tier

3. **mistralai/Mistral-7B-Instruct-v0.3**
   - 7B параметров
   - От Mistral AI
   - Хорошо следует инструкциям

### Специализированные модели

1. **deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B**
   - Reasoning модель
   - Хороша для логических задач

2. **HuggingFaceH4/zephyr-7b-beta**
   - Диалоговая модель
   - Дружелюбные ответы

## ❌ Модели которые НЕ работают

Эти модели могут быть недоступны через Router API с бесплатным tier:

- ❌ `microsoft/Phi-3.5-mini-instruct` - не поддерживается провайдерами
- ❌ `facebook/opt-125m` - старый формат API
- ❌ `gpt2`, `distilgpt2` - устаревшие ID без организации

## Проверенные пары для сравнения

### Рекомендуемая пара (по умолчанию)
```kotlin
model1Id = "meta-llama/Llama-3.2-3B-Instruct"
model2Id = "Qwen/Qwen2.5-3B-Instruct"
```
**Почему:** Обе модели быстрые, качественные, разные архитектуры.

### Скорость vs Качество
```kotlin
model1Id = "meta-llama/Llama-3.2-1B-Instruct"  // Быстрая
model2Id = "meta-llama/Llama-3.2-3B-Instruct"  // Качественная
```

### Разные поколения Llama
```kotlin
model1Id = "meta-llama/Llama-3.1-8B-Instruct"  // Старое поколение
model2Id = "meta-llama/Llama-3.2-3B-Instruct"  // Новое поколение
```

### Разные провайдеры/архитектуры
```kotlin
model1Id = "meta-llama/Llama-3.2-3B-Instruct"  // Meta
model2Id = "Qwen/Qwen2.5-3B-Instruct"          // Alibaba
```

### Для мультиязычности
```kotlin
model1Id = "Qwen/Qwen2.5-3B-Instruct"     // Отлично для русского/китайского
model2Id = "google/gemma-2-2b-it"         // Более англоцентрична
```

## Формат запроса

Все эти модели работают с одинаковым форматом:

```json
{
    "model": "meta-llama/Llama-3.2-3B-Instruct",
    "messages": [
        {
            "role": "user",
            "content": "Your prompt here"
        }
    ],
    "max_tokens": 256,
    "temperature": 0.7,
    "top_p": 0.9,
    "stream": false
}
```

## Оптимальные параметры

### Для креативных задач
```kotlin
maxTokens = 512
temperature = 0.9
topP = 0.95
```

### Для фактических ответов
```kotlin
maxTokens = 256
temperature = 0.3
topP = 0.9
```

### Для кода
```kotlin
maxTokens = 1024
temperature = 0.2
topP = 0.95
```

## Как проверить новую модель

1. Откройте https://huggingface.co/models
2. Найдите интересующую модель
3. Проверьте наличие badge "Inference API"
4. Попробуйте в веб-виджете на странице модели
5. Если работает - можно использовать в приложении

## Обработка ошибок

### "model_not_supported"
Означает что модель недоступна через Router API для вашего аккаунта.

**Решение:**
- Попробуйте другую модель из проверенного списка
- Проверьте план на Hugging Face (может требовать Pro)
- Убедитесь что используете правильный ID

### "Model is loading"
Модель "прогревается" на сервере.

**Решение:**
- Подождите 20-30 секунд
- Повторите запрос
- Последующие запросы будут быстрее

## Советы по выбору модели

### Выбирайте по размеру:
- **1B-2B:** Для быстрых ответов, простых задач
- **3B:** Оптимальный баланс (рекомендуется)
- **7B+:** Для сложных задач, но медленнее

### Выбирайте по задаче:
- **Универсальные:** Llama 3.2, Qwen 2.5
- **Диалог:** Zephyr, DialoGPT
- **Код:** CodeLlama, StarCoder
- **Мультиязычные:** Qwen 2.5, mGPT

### Выбирайте по скорости:
- **Самые быстрые:** 1B модели
- **Средние:** 3B модели
- **Медленные но качественные:** 7B+ модели

## Лимиты бесплатного tier

- **Rate limit:** ~10-30 запросов/минуту
- **Запросы/день:** ~1000-5000 (зависит от нагрузки)
- **Max tokens:** Обычно до 4096, но рекомендуется 256-512

## Обновления

Этот список регулярно обновляется. Проверяйте актуальность моделей на:
https://huggingface.co/models?pipeline_tag=text-generation&inference=warm

**Последнее обновление:** Январь 2025

## Быстрая замена в коде

Чтобы изменить модель, отредактируйте `ModelComparisonViewModel.kt`:

```kotlin
// Строки 34-35
private var model1Id = "meta-llama/Llama-3.2-3B-Instruct"
private var model2Id = "Qwen/Qwen2.5-3B-Instruct"
```

Замените на любую модель из проверенного списка выше.